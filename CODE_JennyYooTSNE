# https://www.nltk.org/howto/collocations.html

from __future__ import print_function
import nltk
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk.collocations 
import xml.etree.ElementTree as ET
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation 
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap


# Corpus
directory_path = '/Users/osuhyeon/NLTK_court_document/'
Case1_file_path = ['Adidas Complaint 2015-9-14',  
                    'Adidas Opinion and Order 2016-02-12', 
                    'Adidas Opinion and Order 2016-04-16',
                    'Adidas Opinion and Order 2017-08-03',
                    'Adidas Court Opinion 2018-3-10'
                   ]
Case2_file_path = ['JennyYoo_Complaint_2018-10-26', 
                    'JennyYoo Discovery Order 2019-12-16'
                   ]
Case3_file_path = ['PUMA Complaint 2017-3-31'
                  ]

path_list = [directory_path + file + '.txt' for file in Case2_file_path]


# Dictionary
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['-', 'et', 'al', 'llc', 'inc', 'called', 'would', 'have'])
corpus = []

for file in path_list:
    line = open(file, 'r').readlines()
    lines = [li.lower() for li in line if li not in stopwords]
    linew = [re.sub('r[^A-Za-z]+', ' ', li) for li in lines]
    linew2 = [re.sub('jy', 'Jenny Yoo', li2) for li2 in linew]
    linew3 = [re.sub('db', "David's Bridal", li3) for li3 in linew2]

    lemmas = [WordNetLemmatizer().lemmatize(w, 'v') 
              for w in linew3 
              if w not in stopwords]    

    corpus.extend(lemmas)

# Words_Collocation
# Split Train Data, Test Data
# n_features = numbers of column in LDA, least common multiple 360 of 12,15,18
n_top_words = 20
n_features = 720 

train_data = corpus[:][:]
print("train_data: %s" % type(train_data))

# LDA
# LDA uses Term Frequency = CountVectorizer
# (LDA) Train Data : fit and transform
# (LDA) Test Data : unfit and transform
# n_component = number of topics

# (TF) Train Data : fit and transform
# (TF) Test Data : unfit and transform

# CountVectorizer_ component : topic
# CountVectorizer_ feature : term frequency

n_components = 6
ngram_tf = CountVectorizer(stop_words = stopwords, ngram_range=(2,2), max_features = n_features)
ngram_train_fit_transformed = ngram_tf.fit_transform(train_data).reshape(-1, n_components)
print("ngram_train_fit_transformed: %s" % type(ngram_train_fit_transformed))
print("Fitting LDA models with tf features,", " n_components = %d, n_features = %d" % (n_components, n_features))
 
   
lda = LatentDirichletAllocation(
    n_components = n_components, 
    learning_method = 'online', 
    random_state = 0)

lda_train_fit_transformed  = lda.fit_transform(ngram_train_fit_transformed)
lda_train_perplexity = lda.perplexity(lda_train_fit_transformed)

print('sklearn perplexity: %3f' % (lda_train_perplexity))
print(np.shape(lda_train_fit_transformed))

word_idx, topic  = np.shape(lda_train_fit_transformed)
print("topic_idx: %d" % word_idx)

"""
each_ngram_word = ngram_tf.get_feature_names()
for topic_idx, topic in enumerate(lda_train_fit_transformed.components_):
    print("\n Topic %d:" % topic_idx)
    print(",".join(each_ngram_word[i] for i in topic.argsort()[:-(n_features+1):-1]))
"""

# TSNE
# n_component = number of dimensions to compress
# Consider n_iter following Iteration 50 ~ 1000 results!
tsne_model = TSNE(
    n_components = 3, 
    init = 'random', 
    perplexity = lda_train_perplexity, 
    verbose = 2, 
    random_state = 0,
    n_iter = 450)
tsne_y = tsne_model.fit_transform(lda_train_fit_transformed)
print("tsne_y:" np.shape(tsne_y))


# pandas DataFrame by "color"
df = pd.DataFrame(tsne_y)
# df['topic'] = lda_train_fit_transformed.argmax(axis=1)
df.columns = ['TSNE1', 'TSNE2', 'TSNE3'] #'topic'


# Draw 3d scatter plot
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1, projection = '3d')
cmap = ListedColormap(sns.color_palette("husl", 256).as_hex())

x = df['TSNE1']
y = df['TSNE2']
z = df['TSNE3']
ax.scatter(x, y, z)

ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
# ax.legend()
plt.title('TSNE plot of Latent Dirichlet Allocation')
plt.suptitle("Jenny Yoo", fontsize = 9)
plt.show()


"""
[t-SNE] Computed conditional probabilities for sample 50000 / 68040
[t-SNE] Computed conditional probabilities for sample 51000 / 68040
[t-SNE] Computed conditional probabilities for sample 52000 / 68040
[t-SNE] Computed conditional probabilities for sample 53000 / 68040
[t-SNE] Computed conditional probabilities for sample 54000 / 68040
[t-SNE] Computed conditional probabilities for sample 55000 / 68040
[t-SNE] Computed conditional probabilities for sample 56000 / 68040
[t-SNE] Computed conditional probabilities for sample 57000 / 68040
[t-SNE] Computed conditional probabilities for sample 58000 / 68040
[t-SNE] Computed conditional probabilities for sample 59000 / 68040
[t-SNE] Computed conditional probabilities for sample 60000 / 68040
[t-SNE] Computed conditional probabilities for sample 61000 / 68040
[t-SNE] Computed conditional probabilities for sample 62000 / 68040
[t-SNE] Computed conditional probabilities for sample 63000 / 68040
[t-SNE] Computed conditional probabilities for sample 64000 / 68040
[t-SNE] Computed conditional probabilities for sample 65000 / 68040
[t-SNE] Computed conditional probabilities for sample 66000 / 68040
[t-SNE] Computed conditional probabilities for sample 67000 / 68040
[t-SNE] Computed conditional probabilities for sample 68000 / 68040
[t-SNE] Computed conditional probabilities for sample 68040 / 68040
[t-SNE] Mean sigma: 0.000000
[t-SNE] Computed conditional probabilities in 12.693s
[t-SNE] Iteration 50: error = 93.0391159, gradient norm = 0.2388954 (50 iterations in 200.215s)
[t-SNE] Iteration 100: error = 106.7028580, gradient norm = 0.2713487 (50 iterations in 206.945s)
[t-SNE] Iteration 150: error = 122.5393295, gradient norm = 0.2079105 (50 iterations in 360.495s)
[t-SNE] Iteration 200: error = 130.9523621, gradient norm = 0.2144901 (50 iterations in 213.086s)
[t-SNE] Iteration 250: error = 139.0306244, gradient norm = 0.1821893 (50 iterations in 199.233s)
[t-SNE] KL divergence after 250 iterations with early exaggeration: 139.030624
[t-SNE] Iteration 300: error = 9.3156710, gradient norm = 0.0058127 (50 iterations in 183.439s)
[t-SNE] Iteration 350: error = 8.9755688, gradient norm = 0.0034560 (50 iterations in 122.056s)
[t-SNE] Iteration 400: error = 7.6144915, gradient norm = 0.0022601 (50 iterations in 100.320s)
[t-SNE] Iteration 450: error = 7.1260004, gradient norm = 0.0008733 (50 iterations in 114.284s)
[t-SNE] KL divergence after 450 iterations: 7.126000
(68040, 3)
"""