# LDA
# LDA uses Term Frequency = CountVectorizer
# (LDA) Train Data : fit and transform
# (LDA) Test Data : unfit and transform
# n_component = number of topics

# (TF) Train Data : fit and transform
# (TF) Test Data : unfit and transform

# CountVectorizer_ component : topic
# CountVectorizer_ feature : term frequency

n_components = 6
ngram_tf = CountVectorizer(stop_words = stopwords, ngram_range=(2,2), max_features = n_features)
ngram_train_fit_transformed = ngram_tf.fit_transform(train_data).reshape(-1, n_components)
print("Fitting LDA models with tf features,", " n_components = %d, n_features = %d" % (n_components, n_features))
    
lda = LatentDirichletAllocation(
    n_components = n_components, 
    learning_method = 'online', 
    random_state = 0)

lda_train_fit_transformed  = lda.fit_transform(ngram_train_fit_transformed)
lda_train_perplexity = lda.perplexity(lda_train_fit_transformed)
print('sklearn perplexity: train = %3f' % (lda_train_perplexity))

print(np.shape(lda_train_fit_transformed))
topic_idx, topic  = np.shape(lda_train_fit_transformed)
print("topic_idx: %d" % topic_idx)


# TSNE
# n_component = number of dimensions to compress
# Consider n_iter following Iteration 50 ~ 1000 results!
tsne_model = TSNE(
    n_components = 3, 
    init = 'random', 
    perplexity = lda_train_perplexity, 
    verbose = 2, 
    random_state = 0,
    n_iter = 250)
tsne_y = tsne_model.fit_transform(lda_train_fit_transformed)
print(np.shape(tsne_y))

colormap = np.array([
    ["#1f77b4", "#aec7e8", "#ff7f0e", "#ffbb78", "#2ca02c"],
    ["#98df8a", "#d62728", "#ff9896", "#9467bd", "#c5b0d5"],
    ["#8c564b", "#c49c94", "#e377c2", "#f7b6d2", "#7f7f7f"],
    ["#c7c7c7", "#bcbd22", "#dbdb8d", "#17becf", "#9edae5"]
    ])

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1, projection = '3d')

for idxs in topic_idx:
    
X = tsne_y[:,0]
Y = tsne_y[:,1]
Z = tsne_y[:,2]
ax.scatter(X, Y, Z)

ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
plt.suptitle("Result of tsne_y", fontsize = 9)
plt.show()

